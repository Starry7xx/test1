import torch
from torch.utils.data import DataLoader
from dataset import RegressionDataset
from model.models import RegressionModel, RegressionModel2
import numpy as np
import pandas as pd
import os, yaml, pickle, glob
from transformers import RobertaTokenizerFast
import tqdm
import argparse
# --- [ä¿®å¤] æ·»åŠ  datetime å¯¼å…¥ ---
from datetime import datetime


# -------------------------------


class InferenceScaler:
    def __init__(self, mean=None, std=None):
        self.mean = mean
        self.std = std

    def inverse_transform(self, normalized_preds):
        if self.mean is None: return normalized_preds
        return normalized_preds * self.std + self.mean


def predict_fn(data_loader, model, device):
    model.eval()
    predictions = []
    with torch.no_grad():
        for batch in tqdm.tqdm(data_loader, desc="   Inferring", leave=False):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(batch)
            if outputs.shape[-1] == 1: outputs = outputs.squeeze(-1)
            predictions.extend(outputs.cpu().numpy())
    return np.array(predictions)


def run_ensemble_prediction(data_path, pt_ckpt_dir_path, save_path, tag, debug=False):
    print("=============================================================")
    print(f"Ensemble Prediction scanning: {pt_ckpt_dir_path}")
    print("=============================================================")

    # 1. æ‰«ææ‰€æœ‰æ¨¡å‹æƒé‡
    seed_ckpts = glob.glob(os.path.join(pt_ckpt_dir_path, "best_model_seed_*.pt"))
    if len(seed_ckpts) == 0:
        # å›é€€ï¼šå¦‚æœä¸æ˜¯é›†æˆè®­ç»ƒçš„ï¼Œæ‰¾æ™®é€šæƒé‡
        fallback = os.path.join(pt_ckpt_dir_path, "checkpoint.pt")
        # å†æ¬¡å›é€€ï¼šæ‰¾é»˜è®¤å checkpoint.pt
        fallback_default = os.path.join(pt_ckpt_dir_path, "checkpoint.pt")

        if os.path.exists(fallback):
            seed_ckpts = [fallback]
        elif os.path.exists(fallback_default):
            seed_ckpts = [fallback_default]
        else:
            raise FileNotFoundError("No 'best_model_seed_*.pt', 'checkpoint.pt', or 'checkpoint.pt' found!")

    print(f"ğŸ” Found {len(seed_ckpts)} models: {[os.path.basename(x) for x in seed_ckpts]}")

    # 2. å‡†å¤‡æ•°æ®
    df_test = pd.read_pickle(data_path)
    if debug: df_test = df_test.sample(10)

    device = "cuda" if torch.cuda.is_available() and not debug else "cpu"
    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

    # æ£€æŸ¥ target åˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºå ä½ç¬¦
    if "target" in df_test.columns:
        targets_placeholder = df_test["target"].values
        # ç®€å•çš„ç»´åº¦æ£€æŸ¥ï¼Œç¡®ä¿æ˜¯ numpy array
        if isinstance(targets_placeholder[0], (list, np.ndarray)):
            targets_placeholder = np.stack(targets_placeholder)
    else:
        targets_placeholder = np.zeros((len(df_test), 2))

    test_ds = RegressionDataset(texts=df_test["text"].values, targets=targets_placeholder,
                                tokenizer=tokenizer, seq_len=tokenizer.model_max_length)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2)

    # 3. åŠ è½½ Config
    model_config_path = os.path.join(pt_ckpt_dir_path, "clip.yml")
    if not os.path.exists(model_config_path):
        # å°è¯•åœ¨ä¸Šçº§ç›®å½•å¯»æ‰¾ (å…¼å®¹ä¸åŒçš„ç›®å½•ç»“æ„)
        model_config_path = os.path.join(os.path.dirname(pt_ckpt_dir_path), "clip.yml")
        if not os.path.exists(model_config_path):
            model_config_path = "model/clip.yml"  # æœ€åå°è¯•é»˜è®¤è·¯å¾„

    with open(model_config_path, "r") as f:
        model_config = yaml.safe_load(f)

    # 4. å¾ªç¯é¢„æµ‹å¹¶ç´¯åŠ 
    accumulated_preds = None
    scaler = None  # åªéœ€è¯»å–ä¸€æ¬¡ scaler (æ‰€æœ‰ç§å­ scaler æ˜¯ä¸€æ ·çš„)

    for i, ckpt_path in enumerate(seed_ckpts):
        print(f"ğŸ¤– Model {i + 1}/{len(seed_ckpts)}: {os.path.basename(ckpt_path)}")

        # åŠ è½½æƒé‡
        try:
            checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        except TypeError:
            checkpoint = torch.load(ckpt_path, map_location=device)

        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint

        # åˆå§‹åŒ– Scaler (ç¬¬ä¸€æ¬¡æ—¶ï¼Œä¸”checkpointé‡Œæœ‰ç»Ÿè®¡é‡)
        if scaler is None:
            if 'stats' in checkpoint:  # å…¼å®¹æ–°ç‰ˆ regress_run_optimized.py
                scaler = InferenceScaler(mean=checkpoint['stats']['mean'], std=checkpoint['stats']['std'])
                print("   Loaded scaler stats from checkpoint['stats']")
            elif 'scaler_state_dict' in checkpoint:  # å…¼å®¹æ—§ç‰ˆå‘½å
                scaler = InferenceScaler()
                scaler.mean = checkpoint['scaler_state_dict']['mean']
                scaler.std = checkpoint['scaler_state_dict']['std']
                print("   Loaded scaler stats from checkpoint['scaler_state_dict']")
            # å°è¯•ä»ç›®å½•ä¸‹çš„ pkl æ–‡ä»¶åŠ è½½
            elif os.path.exists(os.path.join(pt_ckpt_dir_path, 'scaler_stats.pkl')):
                with open(os.path.join(pt_ckpt_dir_path, 'scaler_stats.pkl'), 'rb') as f:
                    stats = pickle.load(f)
                scaler = InferenceScaler(mean=stats['mean'], std=stats['std'])
                print("   Loaded scaler stats from scaler_stats.pkl file")

        # åˆå§‹åŒ–æ¨¡å‹
        if any("regresshead" in k for k in state_dict.keys()):
            model = RegressionModel2(model_config).to(device)
        else:
            model = RegressionModel(model_config).to(device)

        model.load_state_dict(state_dict, strict=True)

        # é¢„æµ‹
        preds = predict_fn(test_loader, model, device)

        if accumulated_preds is None:
            accumulated_preds = preds
        else:
            accumulated_preds += preds

    # 5. å–å¹³å‡
    avg_preds = accumulated_preds / len(seed_ckpts)

    # 6. åæ ‡å‡†åŒ–
    if scaler:
        print("âœ… Applying inverse transform (Denormalization)")
        final_preds = scaler.inverse_transform(avg_preds)
    else:
        print("âš ï¸ No scaler found, using raw outputs (Assuming model output is already in eV)")
        final_preds = avg_preds

    # 7. ä¿å­˜ä¸è¯„ä¼°
    if not os.path.exists(save_path): os.makedirs(save_path)
    save_file = os.path.join(save_path, f"ENSEMBLE-{tag}.pkl")

    # ä¿å­˜å­—å…¸
    with open(save_file, "wb") as f:
        pickle.dump(dict(zip(df_test["id"].values, final_preds)), f)
    print(f"ğŸ’¾ Predictions saved to: {save_file}")

    if "target" in df_test.columns:
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        try:
            # ç¡®ä¿ targets æ˜¯ numpy æ•°ç»„ä¸”å½¢çŠ¶åŒ¹é…
            targets_val = df_test["target"].values
            if isinstance(targets_val[0], (list, np.ndarray)):
                targets = np.stack(targets_val)
            else:
                targets = targets_val.reshape(-1, 1)

            if final_preds.shape == targets.shape:
                print("\nğŸ“Š Ensemble Evaluation:")
                tasks = ["Adsorption Energy", "d-band Center"]
                for i in range(targets.shape[1]):
                    if i < len(tasks):
                        task_name = tasks[i]
                    else:
                        task_name = f"Task {i + 1}"

                    r2 = r2_score(targets[:, i], final_preds[:, i])
                    mae = mean_absolute_error(targets[:, i], final_preds[:, i])
                    rmse = np.sqrt(mean_squared_error(targets[:, i], final_preds[:, i]))
                    print(f"   {task_name}: R2 = {r2:.4f} | MAE = {mae:.4f} | RMSE = {rmse:.4f}")
            else:
                print(f"\nâš ï¸ Shape mismatch for evaluation: Preds {final_preds.shape} vs Targets {targets.shape}")
        except Exception as e:
            print(f"\nâš ï¸ Evaluation skipped due to error: {e}")
            pass


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--pt_ckpt_dir_path", type=str, required=True)
    parser.add_argument("--save_path", type=str, required=True)
    parser.add_argument("--tag", type=str, default=None)
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()

    if args.tag is None: args.tag = datetime.now().strftime("%m%d_%H%M")
    run_ensemble_prediction(args.data_path, args.pt_ckpt_dir_path, args.save_path, args.tag, args.debug)